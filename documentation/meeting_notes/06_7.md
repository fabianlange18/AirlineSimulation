# Meeting 7. Juni

Paper zu Airline Revenue Management:
https://link.springer.com/article/10.1057/s41272-020-00228-4

# Fortschritt seit dem 7. Juni

- Backward Induction, Policy Iteration und Value Iteration laufen für einfaches Beispiel und liefern die gleiche Policy

- Tabular ADP und Q-Learning funktionieren für das Environment
  - Hyperparameter bislang konstant, Möglichkeit: linearer Schedule
  - Initialisierung der Tabellen mit Nullen, Zufällige Werte bringen bessere Policy bei ADP bei benachbarten Werten des am häufigsten gewählten Pfads

- Alle Stable Baslines Methoden können verwendet werden
  - DQN, SAC, PPO, andere auch möglich
  - Python3.11 erforderlich + aktuellste stable_baselines3, dafür muss in der setup.py gym==0.21 auf gym<=0.21 geändert werden

- Nächste Schritte:
  - Vergleichsmethode für Policies von ADP und Q-Learning (done)
  - Schreiben einer Methode zum Training eines DL-Algorithmus und Ausgabe der resultierenden Policy (done)
  - Vergleichen der Rewards von einer Policy durch n Simulationen
  - Strukturieren von Versuchen zum Methodenvergleich für die Simulation von verschiedenen Methoden unter Ressourcenknappheit
  - Paper zu Airline Revenue Management lesen

- Mögliche Erweiterungen:
  - Kundenverhalten komplizierter durch verschiedene Kundengruppen (Familie, Business, etc)
  - Action Space mehrdimensional für verschiedene Flüge (nur sinnvoll mit Substitutionseffekten, alle Methoden müssten leicht verändert werden)
  - Vorhersagen des Kundenverhaltens durch Fluggesellschaft